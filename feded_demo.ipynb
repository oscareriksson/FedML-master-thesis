{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89743f4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d014ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bb5e6",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.05\n",
    "n_clients = 2\n",
    "distribution = \"iid\"\n",
    "alpha = 0.1\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "public_batch_size = 64\n",
    "n_samples_public = 400\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae914c",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe2b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Using 2500 training samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_train_data(train_fraction, transform):\n",
    "    \"\"\" Sample a chosen fraction of the training dataset to use.\n",
    "\n",
    "        Parameters:\n",
    "        train_fraction  (float): Fraction of training data to use.\n",
    "        transform       (torchvision.transforms.Compose): Collection of transforms to apply on data.\n",
    "\n",
    "        Returns:\n",
    "        torch.utils.data.Subset: Subset of training data.\n",
    "    \"\"\"\n",
    "    train_data = CIFAR10(\n",
    "        root='data', \n",
    "        train=True, \n",
    "        transform=transform, \n",
    "        download=True)\n",
    "    n_samples = len(train_data.targets)\n",
    "    index_limit = int(train_fraction * n_samples)\n",
    "    chosen_indices = np.random.choice(torch.arange(n_samples), size=index_limit, replace=False)\n",
    "    print(f\"\\nUsing {index_limit} training samples\\n\", flush=True)\n",
    "\n",
    "    return Subset(train_data, chosen_indices)\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "train_data = sample_train_data(train_fraction, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa90bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_data(n_clients, distribution, alpha):\n",
    "    \"\"\" Generate iid client data or non-iid by sampling from Dirichlet distribution.\n",
    "\n",
    "        Parameters:\n",
    "        n_clients       (int): Number of clients.\n",
    "        distribution    (str): Indicator to sample iid or non-iid.\n",
    "        alpha           (float): Concentration parameter for Dirichlet distribution.\n",
    "    \"\"\"\n",
    "    labels = np.array([y for (_, y) in train_data])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    partition_matrix = np.ones((n_classes, n_clients))\n",
    "\n",
    "    # iid: Sample from each class until no samples left.\n",
    "    if distribution == \"iid\":\n",
    "        partition_matrix /= n_clients\n",
    "        local_sets_indices = [np.array([], dtype=int) for _ in range(n_clients)]\n",
    "        clients_iter = np.arange(n_clients)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            class_indices = np.where(labels == i)[0]\n",
    "\n",
    "            clients_iter = clients_iter[::-1]\n",
    "            samples_left = True\n",
    "            while samples_left:\n",
    "                for j in clients_iter:\n",
    "                    if len(class_indices) == 0:\n",
    "                        samples_left = False\n",
    "                        break\n",
    "                    else:\n",
    "                        sample_idx = np.random.choice(len(class_indices))\n",
    "                        local_sets_indices[j] = np.append(local_sets_indices[j], class_indices[sample_idx])\n",
    "                        class_indices = np.delete(class_indices, sample_idx)\n",
    "\n",
    "    # non-iid: Sample from dirichlet distribution.\n",
    "    else:\n",
    "        class_indices = []\n",
    "        for i in range(n_classes):\n",
    "            class_indices.append(np.array(range(len(labels)))[labels == i])\n",
    "        valid_pm = False\n",
    "        while not valid_pm:\n",
    "            partition_matrix = np.random.dirichlet((alpha, )*n_clients, n_classes)\n",
    "            valid_pm = all(np.sum(partition_matrix, axis=0) > 0.01)\n",
    "\n",
    "        local_sets_indices = [[] for _ in range(n_clients)]\n",
    "        for each_class in range(n_classes):\n",
    "            sample_size = len(class_indices[each_class])\n",
    "            for client in range(n_clients):\n",
    "                np.random.shuffle(class_indices[each_class])\n",
    "                local_size = int(np.floor(partition_matrix[each_class, client] * sample_size))\n",
    "                local_sets_indices[client] += list(class_indices[each_class][:local_size])\n",
    "                class_indices[each_class] = class_indices[each_class][local_size:]\n",
    "    \n",
    "    return local_sets_indices\n",
    "\n",
    "\n",
    "def get_train_data_loaders(batch_size):\n",
    "    \"\"\" Get list of client training data loaders.\n",
    "\n",
    "        Parameters:\n",
    "        n_clients       (int): Number of clients.\n",
    "        distribution    (str): iid/non-iid distributed data.\n",
    "        alpha           (float): Concentration parameter for dirichlet distribution.\n",
    "        batch_size      (int): Batch size for loading training data.\n",
    "\n",
    "        Returns List[torch.utils.data.DataLoader]\n",
    "    \"\"\"\n",
    "    client_data_loaders = []\n",
    "    for client_indices in local_sets_indices:\n",
    "            np.random.shuffle(client_indices)\n",
    "            client_data_loaders.append(DataLoader(Subset(train_data, client_indices), batch_size))\n",
    "    return client_data_loaders\n",
    "\n",
    "local_sets_indices = generate_client_data(n_clients, distribution, alpha)\n",
    "client_data_loaders = get_train_data_loaders(train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314392ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def split_test_public(n_samples):\n",
    "    labels = np.array([y for (_, y) in test_data])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    n_samples_per_class = int(n_samples / n_classes)\n",
    "    all_indices = np.arange(len(labels))\n",
    "\n",
    "    public_set_indices = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_indices = np.where(labels == i)[0]\n",
    "        chosen_indices = list(np.random.choice(class_indices, n_samples_per_class, replace=False))\n",
    "        public_set_indices.extend(chosen_indices)\n",
    "\n",
    "    # Fill up to n_samples.\n",
    "    samples_left = n_samples - len(public_set_indices)\n",
    "    indices_left = [x for x in all_indices if x not in public_set_indices]\n",
    "    chosen_indices = list(np.random.choice(indices_left, samples_left, replace=False))\n",
    "    public_set_indices.extend(chosen_indices)\n",
    "\n",
    "    test_set_indices = [x for x in all_indices if x not in public_set_indices]\n",
    "\n",
    "    return Subset(test_data, test_set_indices), Subset(test_data, public_set_indices)\n",
    "\n",
    "test_data = CIFAR10(\n",
    "    root='data', \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    download=True)\n",
    "\n",
    "test_data, public_data = split_test_public(n_samples_public)\n",
    "test_data_loader = DataLoader(test_data, test_batch_size)\n",
    "public_data_loader = DataLoader(public_data, public_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b319b",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "393bf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "    \n",
    "class Mnist_Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Student, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, 1, 2)\n",
    "        self.pool = nn.MaxPool2d(4)\n",
    "        self.fc1 = nn.Linear(8 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a556ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar_Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, 1, 2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Cifar10_Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10_Student, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5, 1, 2)\n",
    "        self.pool = nn.MaxPool2d(4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f6618",
   "metadata": {},
   "source": [
    "## Training clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57ca573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Client 1/2: Epoch 3/3: Train loss: 2.299\n",
      "\n",
      "Client 2/2: Epoch 3/3: Train loss: 2.300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_model = Cifar_Cnn()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "logits_ensemble = torch.zeros(n_samples_public, 10)\n",
    "print(\"Starting training\")\n",
    "\n",
    "for i in range(n_clients):\n",
    "    model = copy.deepcopy(local_model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for x, y in client_data_loaders[i]:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            error = loss_function(output, y)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(error.item())\n",
    "            print(f\"Client {i+1}/{n_clients}: \"\n",
    "                  f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "                  f\"Train loss: {sum(train_loss)/len(train_loss):.3f}\", end=\"\\r\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    logits_local = None\n",
    "    with torch.no_grad():\n",
    "        for x, _ in public_data_loader:\n",
    "            if logits_local is None:\n",
    "                logits_local = F.softmax(model(x), dim=1)\n",
    "            else:\n",
    "                logits_local = torch.cat((logits_local, F.softmax(model(x), dim=1)))\n",
    "        \n",
    "    # Increment average\n",
    "    logits_ensemble = logits_ensemble + (logits_local-logits_ensemble)/(i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10836e",
   "metadata": {},
   "source": [
    "## Training student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19ad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_targets = torch.zeros(len(test_data.dataset), 10)\n",
    "for i in range(n_samples_public):\n",
    "    idx_public = public_data.indices[i]\n",
    "    student_targets[idx_public] = logits_ensemble[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a92edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833]],\n",
       " \n",
       "          [[ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833]],\n",
       " \n",
       "          [[ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8088,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8215],\n",
       "           [ 2.8088,  2.7706,  2.7833,  ...,  2.7833,  2.7833,  2.7833]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4922,  0.5559,  0.5686,  ...,  0.9123,  0.9377,  0.9759],\n",
       "           [ 0.1740,  0.1613,  0.1231,  ...,  0.3395,  0.3140,  0.4159],\n",
       "           [ 0.4540,  0.3649,  0.2886,  ...,  0.4413,  0.4159,  0.4668],\n",
       "           ...,\n",
       "           [ 1.0268,  1.0013,  1.0904,  ...,  1.0650,  1.0777,  1.1159],\n",
       "           [ 1.0523,  1.0395,  1.0395,  ...,  1.1541,  1.1541,  1.1923],\n",
       "           [ 1.1668,  1.1668,  1.1414,  ...,  1.1795,  1.1668,  1.2050]],\n",
       " \n",
       "          [[ 0.8995,  0.9504,  0.9504,  ...,  1.2941,  1.3196,  1.3577],\n",
       "           [ 0.6195,  0.5940,  0.5559,  ...,  0.8232,  0.7977,  0.8995],\n",
       "           [ 0.8741,  0.7722,  0.7086,  ...,  0.9632,  0.9504,  0.9886],\n",
       "           ...,\n",
       "           [ 1.1668,  1.1414,  1.2305,  ...,  1.2559,  1.2559,  1.3196],\n",
       "           [ 1.1923,  1.1795,  1.1923,  ...,  1.3450,  1.3450,  1.3705],\n",
       "           [ 1.3196,  1.3068,  1.2814,  ...,  1.3705,  1.3577,  1.3959]],\n",
       " \n",
       "          [[ 0.6704,  0.7722,  0.7850,  ...,  1.0141,  1.0395,  1.0523],\n",
       "           [ 0.6195,  0.6195,  0.5940,  ...,  0.6577,  0.6450,  0.7086],\n",
       "           [ 0.8104,  0.7341,  0.7086,  ...,  0.7977,  0.7850,  0.8232],\n",
       "           ...,\n",
       "           [ 0.3268,  0.2886,  0.3649,  ...,  0.4286,  0.4286,  0.4795],\n",
       "           [ 0.3140,  0.3268,  0.3268,  ...,  0.4922,  0.4922,  0.5177],\n",
       "           [ 0.4031,  0.4540,  0.4413,  ...,  0.4922,  0.4922,  0.5177]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6831,  0.5813,  0.4668,  ...,  0.7086,  0.7595,  0.6322],\n",
       "           [ 0.6831,  0.5049,  0.3395,  ...,  0.4286,  0.3649,  0.1486],\n",
       "           [ 0.8486,  0.8486,  0.8104,  ...,  0.3395,  0.2504,  0.2122],\n",
       "           ...,\n",
       "           [ 0.4668,  0.5431,  0.5940,  ...,  0.4540,  0.4031,  0.3395],\n",
       "           [ 0.4922,  0.5813,  0.6195,  ...,  0.6704,  0.6322,  0.5940],\n",
       "           [ 0.9250,  0.9886,  1.0013,  ...,  1.0523,  1.0395,  1.0268]],\n",
       " \n",
       "          [[ 0.8486,  0.7468,  0.6322,  ...,  0.9123,  0.9504,  0.8486],\n",
       "           [ 0.7850,  0.6068,  0.4668,  ...,  0.5940,  0.5304,  0.3268],\n",
       "           [ 0.8868,  0.8995,  0.8868,  ...,  0.4795,  0.3904,  0.3649],\n",
       "           ...,\n",
       "           [ 0.2377,  0.3268,  0.3777,  ...,  0.3395,  0.2758,  0.2122],\n",
       "           [ 0.3522,  0.4413,  0.4795,  ...,  0.5686,  0.5304,  0.4922],\n",
       "           [ 0.8868,  0.9504,  0.9632,  ...,  0.9886,  0.9632,  0.9504]],\n",
       " \n",
       "          [[ 1.0777,  0.9632,  0.8613,  ...,  1.3450,  1.3959,  1.2814],\n",
       "           [ 1.0268,  0.8104,  0.6195,  ...,  1.0395,  0.9759,  0.7722],\n",
       "           [ 1.1414,  1.0777,  0.9886,  ...,  0.9250,  0.8232,  0.7977],\n",
       "           ...,\n",
       "           [-0.0551,  0.0340,  0.0849,  ...,  0.2122,  0.1613,  0.0722],\n",
       "           [ 0.2377,  0.3268,  0.3649,  ...,  0.4668,  0.4286,  0.3777],\n",
       "           [ 0.8741,  0.9377,  0.9504,  ...,  0.8995,  0.8741,  0.8613]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.2432,  1.2814,  1.3068,  ...,  1.5232,  1.5487,  1.5487],\n",
       "           [ 1.2050,  1.2814,  1.2814,  ...,  1.5105,  1.5232,  1.5359],\n",
       "           [ 1.1795,  1.2432,  1.2559,  ...,  1.5105,  1.5232,  1.5232],\n",
       "           ...,\n",
       "           [ 0.1740,  0.0849,  0.0467,  ..., -0.2206, -0.3351, -0.3988],\n",
       "           [ 0.2631,  0.1867,  0.1231,  ..., -0.1187, -0.1569, -0.2969],\n",
       "           [ 0.1104,  0.0976,  0.1231,  ..., -0.1315, -0.0806, -0.0678]],\n",
       " \n",
       "          [[ 0.9123,  0.9886,  1.0395,  ...,  1.3450,  1.3450,  1.3577],\n",
       "           [ 0.8741,  0.9759,  1.0013,  ...,  1.3196,  1.3323,  1.3577],\n",
       "           [ 0.8486,  0.9377,  0.9504,  ...,  1.3196,  1.3323,  1.3577],\n",
       "           ...,\n",
       "           [ 0.1358,  0.0595,  0.0340,  ..., -0.2206, -0.3351, -0.3988],\n",
       "           [ 0.1995,  0.1358,  0.0849,  ..., -0.1060, -0.1442, -0.2715],\n",
       "           [ 0.0595,  0.0595,  0.1104,  ..., -0.1187, -0.0678, -0.0551]],\n",
       " \n",
       "          [[ 0.5686,  0.6450,  0.7086,  ...,  1.0395,  1.0904,  1.0904],\n",
       "           [ 0.5304,  0.6450,  0.6577,  ...,  1.0395,  1.0777,  1.1032],\n",
       "           [ 0.5049,  0.5940,  0.6068,  ...,  1.0141,  1.0650,  1.1032],\n",
       "           ...,\n",
       "           [ 0.0085, -0.0296, -0.0678,  ..., -0.2587, -0.3606, -0.3988],\n",
       "           [ 0.0849,  0.0467, -0.0296,  ..., -0.1696, -0.1951, -0.2842],\n",
       "           [-0.0169, -0.0296,  0.0213,  ..., -0.1824, -0.1315, -0.1060]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           ...,\n",
       "           [ 2.6433,  2.6306,  2.5160,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6306,  2.6306,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6178,  2.6051,  ...,  2.6178,  2.6178,  2.6433]],\n",
       " \n",
       "          [[ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           ...,\n",
       "           [ 2.6433,  2.6306,  2.5160,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6306,  2.6306,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6178,  2.6051,  ...,  2.6178,  2.6178,  2.6433]],\n",
       " \n",
       "          [[ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           [ 1.0013,  0.9886,  0.9886,  ...,  0.9886,  0.9886,  0.9886],\n",
       "           ...,\n",
       "           [ 2.6433,  2.6306,  2.5160,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6306,  2.6306,  ...,  2.6178,  2.6178,  2.6433],\n",
       "           [ 2.6433,  2.6178,  2.6051,  ...,  2.6178,  2.6178,  2.6433]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8088,  2.7578,  2.7706,  ...,  2.7706,  2.7706,  2.7706],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8088]],\n",
       " \n",
       "          [[ 2.8088,  2.7578,  2.7706,  ...,  2.7706,  2.7706,  2.7706],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8088]],\n",
       " \n",
       "          [[ 2.8088,  2.7578,  2.7706,  ...,  2.7706,  2.7706,  2.7706],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8215],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           ...,\n",
       "           [ 2.8215,  2.8088,  2.8088,  ...,  2.8088,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.8088,  2.8215,  ...,  2.8215,  2.8215,  2.8088],\n",
       "           [ 2.8215,  2.7960,  2.8088,  ...,  2.8088,  2.8088,  2.8088]]]]),\n",
       " tensor([[0.1169, 0.0962, 0.0963, 0.1037, 0.1057, 0.1085, 0.0914, 0.0934, 0.0911,\n",
       "          0.0967],\n",
       "         [0.1115, 0.0952, 0.1006, 0.1039, 0.1062, 0.1109, 0.0901, 0.0928, 0.0932,\n",
       "          0.0956],\n",
       "         [0.1122, 0.0953, 0.0989, 0.1045, 0.1065, 0.1094, 0.0901, 0.0937, 0.0935,\n",
       "          0.0960],\n",
       "         [0.1126, 0.0946, 0.0996, 0.1053, 0.1063, 0.1107, 0.0904, 0.0929, 0.0934,\n",
       "          0.0943],\n",
       "         [0.1128, 0.0952, 0.0982, 0.1047, 0.1057, 0.1102, 0.0903, 0.0937, 0.0932,\n",
       "          0.0959],\n",
       "         [0.1126, 0.0942, 0.0990, 0.1056, 0.1072, 0.1101, 0.0900, 0.0927, 0.0936,\n",
       "          0.0951],\n",
       "         [0.1145, 0.0959, 0.0976, 0.1038, 0.1072, 0.1085, 0.0907, 0.0935, 0.0921,\n",
       "          0.0962],\n",
       "         [0.1130, 0.0963, 0.0982, 0.1026, 0.1063, 0.1082, 0.0916, 0.0943, 0.0922,\n",
       "          0.0974],\n",
       "         [0.1149, 0.0967, 0.0960, 0.1032, 0.1067, 0.1076, 0.0911, 0.0940, 0.0924,\n",
       "          0.0972],\n",
       "         [0.1137, 0.0944, 0.0991, 0.1047, 0.1066, 0.1096, 0.0909, 0.0923, 0.0927,\n",
       "          0.0959],\n",
       "         [0.1135, 0.0957, 0.0985, 0.1039, 0.1059, 0.1093, 0.0917, 0.0939, 0.0923,\n",
       "          0.0953],\n",
       "         [0.1143, 0.0988, 0.0966, 0.1025, 0.1054, 0.1088, 0.0905, 0.0959, 0.0911,\n",
       "          0.0960],\n",
       "         [0.1163, 0.0967, 0.0955, 0.1020, 0.1066, 0.1078, 0.0925, 0.0951, 0.0909,\n",
       "          0.0967],\n",
       "         [0.1126, 0.0953, 0.0999, 0.1049, 0.1078, 0.1089, 0.0900, 0.0926, 0.0932,\n",
       "          0.0949],\n",
       "         [0.1120, 0.0956, 0.0989, 0.1045, 0.1071, 0.1088, 0.0899, 0.0932, 0.0945,\n",
       "          0.0955],\n",
       "         [0.1141, 0.0962, 0.0964, 0.1038, 0.1070, 0.1084, 0.0913, 0.0943, 0.0922,\n",
       "          0.0963],\n",
       "         [0.1130, 0.0960, 0.0983, 0.1033, 0.1073, 0.1088, 0.0918, 0.0950, 0.0921,\n",
       "          0.0945],\n",
       "         [0.1141, 0.0970, 0.0964, 0.1044, 0.1054, 0.1095, 0.0910, 0.0929, 0.0910,\n",
       "          0.0983],\n",
       "         [0.1129, 0.0955, 0.0994, 0.1047, 0.1083, 0.1094, 0.0896, 0.0920, 0.0929,\n",
       "          0.0952],\n",
       "         [0.1142, 0.0957, 0.0982, 0.1041, 0.1058, 0.1092, 0.0910, 0.0930, 0.0934,\n",
       "          0.0956],\n",
       "         [0.1129, 0.0952, 0.0985, 0.1041, 0.1067, 0.1095, 0.0907, 0.0935, 0.0932,\n",
       "          0.0958],\n",
       "         [0.1144, 0.0963, 0.0966, 0.1029, 0.1067, 0.1072, 0.0923, 0.0949, 0.0921,\n",
       "          0.0965],\n",
       "         [0.1132, 0.0936, 0.0982, 0.1050, 0.1063, 0.1132, 0.0908, 0.0916, 0.0924,\n",
       "          0.0956],\n",
       "         [0.1107, 0.0950, 0.1009, 0.1047, 0.1066, 0.1095, 0.0895, 0.0940, 0.0939,\n",
       "          0.0951],\n",
       "         [0.1132, 0.0964, 0.0983, 0.1031, 0.1067, 0.1068, 0.0908, 0.0927, 0.0919,\n",
       "          0.1001],\n",
       "         [0.1122, 0.0967, 0.0979, 0.1027, 0.1071, 0.1064, 0.0910, 0.0936, 0.0928,\n",
       "          0.0996],\n",
       "         [0.1129, 0.0963, 0.0976, 0.1040, 0.1066, 0.1085, 0.0909, 0.0941, 0.0925,\n",
       "          0.0966],\n",
       "         [0.1112, 0.0981, 0.0969, 0.1047, 0.1062, 0.1088, 0.0902, 0.0947, 0.0929,\n",
       "          0.0962],\n",
       "         [0.1106, 0.0944, 0.1010, 0.1055, 0.1075, 0.1102, 0.0902, 0.0924, 0.0933,\n",
       "          0.0951],\n",
       "         [0.1099, 0.0953, 0.1004, 0.1050, 0.1061, 0.1105, 0.0901, 0.0936, 0.0944,\n",
       "          0.0947],\n",
       "         [0.1128, 0.0947, 0.0997, 0.1049, 0.1065, 0.1100, 0.0903, 0.0922, 0.0933,\n",
       "          0.0956],\n",
       "         [0.1106, 0.0951, 0.1012, 0.1048, 0.1066, 0.1099, 0.0893, 0.0922, 0.0939,\n",
       "          0.0964],\n",
       "         [0.1133, 0.0966, 0.0968, 0.1028, 0.1071, 0.1087, 0.0913, 0.0952, 0.0921,\n",
       "          0.0960],\n",
       "         [0.1135, 0.0962, 0.0969, 0.1039, 0.1070, 0.1085, 0.0897, 0.0929, 0.0929,\n",
       "          0.0984],\n",
       "         [0.1129, 0.0958, 0.0979, 0.1049, 0.1066, 0.1108, 0.0900, 0.0922, 0.0925,\n",
       "          0.0965],\n",
       "         [0.1141, 0.0941, 0.0983, 0.1050, 0.1068, 0.1104, 0.0901, 0.0917, 0.0933,\n",
       "          0.0962],\n",
       "         [0.1150, 0.0962, 0.0972, 0.1032, 0.1080, 0.1073, 0.0909, 0.0940, 0.0923,\n",
       "          0.0958],\n",
       "         [0.1123, 0.0953, 0.0987, 0.1040, 0.1069, 0.1105, 0.0905, 0.0938, 0.0932,\n",
       "          0.0947],\n",
       "         [0.1126, 0.0944, 0.1002, 0.1044, 0.1068, 0.1087, 0.0909, 0.0907, 0.0922,\n",
       "          0.0992],\n",
       "         [0.1128, 0.0953, 0.0988, 0.1050, 0.1076, 0.1087, 0.0897, 0.0923, 0.0939,\n",
       "          0.0959],\n",
       "         [0.1125, 0.0950, 0.0991, 0.1046, 0.1059, 0.1107, 0.0912, 0.0938, 0.0930,\n",
       "          0.0943],\n",
       "         [0.1113, 0.0957, 0.0981, 0.1034, 0.1071, 0.1085, 0.0905, 0.0946, 0.0935,\n",
       "          0.0973],\n",
       "         [0.1117, 0.0944, 0.0995, 0.1062, 0.1066, 0.1107, 0.0897, 0.0923, 0.0934,\n",
       "          0.0954],\n",
       "         [0.1098, 0.0956, 0.1009, 0.1041, 0.1065, 0.1095, 0.0899, 0.0931, 0.0937,\n",
       "          0.0968],\n",
       "         [0.1128, 0.0962, 0.0991, 0.1046, 0.1067, 0.1088, 0.0900, 0.0938, 0.0919,\n",
       "          0.0962],\n",
       "         [0.1109, 0.0950, 0.0999, 0.1042, 0.1069, 0.1106, 0.0901, 0.0949, 0.0944,\n",
       "          0.0934],\n",
       "         [0.1104, 0.0953, 0.1006, 0.1044, 0.1065, 0.1095, 0.0901, 0.0937, 0.0940,\n",
       "          0.0954],\n",
       "         [0.1128, 0.0956, 0.0996, 0.1051, 0.1066, 0.1084, 0.0903, 0.0922, 0.0930,\n",
       "          0.0964],\n",
       "         [0.1103, 0.0960, 0.1007, 0.1053, 0.1075, 0.1084, 0.0896, 0.0937, 0.0944,\n",
       "          0.0942],\n",
       "         [0.1109, 0.0960, 0.0991, 0.1036, 0.1082, 0.1094, 0.0894, 0.0930, 0.0948,\n",
       "          0.0956],\n",
       "         [0.1121, 0.0935, 0.1010, 0.1044, 0.1064, 0.1116, 0.0911, 0.0925, 0.0936,\n",
       "          0.0937],\n",
       "         [0.1101, 0.0938, 0.1021, 0.1064, 0.1066, 0.1118, 0.0895, 0.0915, 0.0937,\n",
       "          0.0946],\n",
       "         [0.1104, 0.0955, 0.1004, 0.1034, 0.1071, 0.1087, 0.0907, 0.0921, 0.0927,\n",
       "          0.0989],\n",
       "         [0.1093, 0.0973, 0.0992, 0.1036, 0.1073, 0.1083, 0.0900, 0.0949, 0.0934,\n",
       "          0.0966],\n",
       "         [0.1118, 0.0950, 0.1002, 0.1047, 0.1075, 0.1087, 0.0897, 0.0917, 0.0939,\n",
       "          0.0967],\n",
       "         [0.1105, 0.0960, 0.1008, 0.1048, 0.1067, 0.1076, 0.0904, 0.0921, 0.0924,\n",
       "          0.0987],\n",
       "         [0.1122, 0.0964, 0.0989, 0.1040, 0.1057, 0.1085, 0.0909, 0.0943, 0.0931,\n",
       "          0.0960],\n",
       "         [0.1123, 0.0950, 0.0995, 0.1057, 0.1077, 0.1098, 0.0889, 0.0920, 0.0943,\n",
       "          0.0948],\n",
       "         [0.1114, 0.0960, 0.0993, 0.1038, 0.1053, 0.1088, 0.0919, 0.0930, 0.0930,\n",
       "          0.0977],\n",
       "         [0.1126, 0.0953, 0.0998, 0.1044, 0.1067, 0.1092, 0.0908, 0.0935, 0.0934,\n",
       "          0.0944],\n",
       "         [0.1098, 0.0957, 0.0999, 0.1047, 0.1062, 0.1101, 0.0898, 0.0935, 0.0946,\n",
       "          0.0958],\n",
       "         [0.1130, 0.0951, 0.0992, 0.1045, 0.1063, 0.1095, 0.0909, 0.0931, 0.0935,\n",
       "          0.0949],\n",
       "         [0.1136, 0.0943, 0.0996, 0.1042, 0.1062, 0.1094, 0.0915, 0.0939, 0.0930,\n",
       "          0.0944],\n",
       "         [0.1131, 0.0993, 0.0974, 0.1029, 0.1060, 0.1068, 0.0911, 0.0953, 0.0912,\n",
       "          0.0969]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StudentData(Dataset):\n",
    "    def __init__(self, dataset, targets):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.targets = targets\n",
    "        self.indices = dataset.indices\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, list):\n",
    "            data, _ = self.dataset[[i for i in index]]\n",
    "            target = self.targets[[self.indices[i] for i in index]]\n",
    "        else:\n",
    "            data, _ = self.dataset[index]\n",
    "            target = self.targets[self.indices[index]]\n",
    "            \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "student_data = StudentData(public_data, student_targets)\n",
    "student_data_loader = DataLoader(student_data, public_batch_size)\n",
    "data_iter = iter(student_data_loader)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "329b1791",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 1, 5, 5], expected input[64, 3, 32, 32] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m student_data_loader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(output, y)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mMnist_Student.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "File \u001b[0;32m~/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ml_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml_env/lib/python3.8/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 1, 5, 5], expected input[64, 3, 32, 32] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "model = Mnist_Student()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs_student = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs_student):\n",
    "    train_loss = []\n",
    "    for x, y in student_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs_student}: \"\n",
    "          f\"Train loss: {sum(train_loss)/len(train_loss):.3f}\", end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32c5544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.762\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "     for x, y in test_data_loader:\n",
    "            output = model(x)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total +=1\n",
    "                \n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f616dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = \"fedavg+feded\"\n",
    "algs = alg.split(\"+\")\n",
    "ens = [\"fede\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6977e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(alg in algs for alg in ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d1d9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4983b2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe9e18530d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_data_loaders[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6713f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
