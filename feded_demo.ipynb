{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89743f4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d014ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bb5e6",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.3\n",
    "n_clients = 5\n",
    "distribution = \"iid\"\n",
    "alpha = 0.1\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "public_batch_size = 64\n",
    "n_samples_public = 400\n",
    "n_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae914c",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe2b2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 18000 training samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_train_data(train_fraction, transform):\n",
    "    \"\"\" Sample a chosen fraction of the training dataset to use.\n",
    "\n",
    "        Parameters:\n",
    "        train_fraction  (float): Fraction of training data to use.\n",
    "        transform       (torchvision.transforms.Compose): Collection of transforms to apply on data.\n",
    "\n",
    "        Returns:\n",
    "        torch.utils.data.Subset: Subset of training data.\n",
    "    \"\"\"\n",
    "    train_data = MNIST(\n",
    "        root='data', \n",
    "        train=True, \n",
    "        transform=transform, \n",
    "        download=True)\n",
    "    n_samples = len(train_data.targets)\n",
    "    index_limit = int(train_fraction * n_samples)\n",
    "    chosen_indices = np.random.choice(torch.arange(n_samples), size=index_limit, replace=False)\n",
    "    print(f\"\\nUsing {index_limit} training samples\\n\", flush=True)\n",
    "\n",
    "    return Subset(train_data, chosen_indices)\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "train_data = sample_train_data(train_fraction, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa90bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_data(n_clients, distribution, alpha):\n",
    "    \"\"\" Generate iid client data or non-iid by sampling from Dirichlet distribution.\n",
    "\n",
    "        Parameters:\n",
    "        n_clients       (int): Number of clients.\n",
    "        distribution    (str): Indicator to sample iid or non-iid.\n",
    "        alpha           (float): Concentration parameter for Dirichlet distribution.\n",
    "    \"\"\"\n",
    "    labels = np.array([y for (_, y) in train_data])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    partition_matrix = np.ones((n_classes, n_clients))\n",
    "\n",
    "    # iid: Sample from each class until no samples left.\n",
    "    if distribution == \"iid\":\n",
    "        partition_matrix /= n_clients\n",
    "        local_sets_indices = [np.array([], dtype=int) for _ in range(n_clients)]\n",
    "        clients_iter = np.arange(n_clients)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            class_indices = np.where(labels == i)[0]\n",
    "\n",
    "            clients_iter = clients_iter[::-1]\n",
    "            samples_left = True\n",
    "            while samples_left:\n",
    "                for j in clients_iter:\n",
    "                    if len(class_indices) == 0:\n",
    "                        samples_left = False\n",
    "                        break\n",
    "                    else:\n",
    "                        sample_idx = np.random.choice(len(class_indices))\n",
    "                        local_sets_indices[j] = np.append(local_sets_indices[j], class_indices[sample_idx])\n",
    "                        class_indices = np.delete(class_indices, sample_idx)\n",
    "\n",
    "    # non-iid: Sample from dirichlet distribution.\n",
    "    else:\n",
    "        class_indices = []\n",
    "        for i in range(n_classes):\n",
    "            class_indices.append(np.array(range(len(labels)))[labels == i])\n",
    "        valid_pm = False\n",
    "        while not valid_pm:\n",
    "            partition_matrix = np.random.dirichlet((alpha, )*n_clients, n_classes)\n",
    "            valid_pm = all(np.sum(partition_matrix, axis=0) > 0.01)\n",
    "\n",
    "        local_sets_indices = [[] for _ in range(n_clients)]\n",
    "        for each_class in range(n_classes):\n",
    "            sample_size = len(class_indices[each_class])\n",
    "            for client in range(n_clients):\n",
    "                np.random.shuffle(class_indices[each_class])\n",
    "                local_size = int(np.floor(partition_matrix[each_class, client] * sample_size))\n",
    "                local_sets_indices[client] += list(class_indices[each_class][:local_size])\n",
    "                class_indices[each_class] = class_indices[each_class][local_size:]\n",
    "    \n",
    "    return local_sets_indices\n",
    "\n",
    "\n",
    "def get_train_data_loaders(batch_size):\n",
    "    \"\"\" Get list of client training data loaders.\n",
    "\n",
    "        Parameters:\n",
    "        n_clients       (int): Number of clients.\n",
    "        distribution    (str): iid/non-iid distributed data.\n",
    "        alpha           (float): Concentration parameter for dirichlet distribution.\n",
    "        batch_size      (int): Batch size for loading training data.\n",
    "\n",
    "        Returns List[torch.utils.data.DataLoader]\n",
    "    \"\"\"\n",
    "    client_data_loaders = []\n",
    "    for client_indices in local_sets_indices:\n",
    "            np.random.shuffle(client_indices)\n",
    "            client_data_loaders.append(DataLoader(Subset(train_data, client_indices), batch_size))\n",
    "    return client_data_loaders\n",
    "\n",
    "local_sets_indices = generate_client_data(n_clients, distribution, alpha)\n",
    "client_data_loaders = get_train_data_loaders(train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314392ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_public(n_samples):\n",
    "    labels = np.array([y for (_, y) in test_data])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    n_samples_per_class = int(n_samples / n_classes)\n",
    "    all_indices = np.arange(len(labels))\n",
    "\n",
    "    public_set_indices = []\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        class_indices = np.where(labels == i)[0]\n",
    "        chosen_indices = list(np.random.choice(class_indices, n_samples_per_class, replace=False))\n",
    "        public_set_indices.extend(chosen_indices)\n",
    "\n",
    "    # Fill up to n_samples.\n",
    "    samples_left = n_samples - len(public_set_indices)\n",
    "    indices_left = [x for x in all_indices if x not in public_set_indices]\n",
    "    chosen_indices = list(np.random.choice(indices_left, samples_left, replace=False))\n",
    "    public_set_indices.extend(chosen_indices)\n",
    "\n",
    "    test_set_indices = [x for x in all_indices if x not in public_set_indices]\n",
    "\n",
    "    return Subset(test_data, test_set_indices), Subset(test_data, public_set_indices)\n",
    "\n",
    "test_data = MNIST(\n",
    "    root='data', \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    download=True)\n",
    "\n",
    "test_data, public_data = split_test_public(n_samples_public)\n",
    "test_data_loader = DataLoader(test_data, test_batch_size)\n",
    "public_data_loader = DataLoader(public_data, public_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b319b",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "393bf38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "    \n",
    "class Mnist_Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mnist_Student, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f6618",
   "metadata": {},
   "source": [
    "## Training clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57ca573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Client 1/5: Epoch 3/3: Train loss: 0.667\n",
      "\n",
      "Client 2/5: Epoch 3/3: Train loss: 0.689\n",
      "\n",
      "Client 3/5: Epoch 3/3: Train loss: 0.711\n",
      "\n",
      "Client 4/5: Epoch 3/3: Train loss: 0.654\n",
      "\n",
      "Client 5/5: Epoch 3/3: Train loss: 0.674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_model = Mnist_Cnn()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "logits_ensemble = torch.zeros(n_samples_public, 10)\n",
    "print(\"Starting training\")\n",
    "\n",
    "for i in range(n_clients):\n",
    "    model = copy.deepcopy(local_model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = []\n",
    "        model.train()\n",
    "        for x, y in client_data_loaders[i]:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            error = loss_function(output, y)\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(error.item())\n",
    "            print(f\"Client {i+1}/{n_clients}: \"\n",
    "                  f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "                  f\"Train loss: {sum(train_loss)/len(train_loss):.3f}\", end=\"\\r\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    logits_local = None\n",
    "    with torch.no_grad():\n",
    "        for x, _ in public_data_loader:\n",
    "            if logits_local is None:\n",
    "                logits_local = F.softmax(model(x), dim=1)\n",
    "            else:\n",
    "                logits_local = torch.cat((logits_local, F.softmax(model(x), dim=1)))\n",
    "        \n",
    "    # Increment average\n",
    "    logits_ensemble = logits_ensemble + (logits_local-logits_ensemble)/(i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10836e",
   "metadata": {},
   "source": [
    "## Training student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19ad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_targets = torch.zeros(len(test_data.dataset), 10)\n",
    "for i in range(n_samples_public):\n",
    "    idx_public = public_data.indices[i]\n",
    "    student_targets[idx_public] = logits_ensemble[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a92edc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       " \n",
       " \n",
       "         [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       " \n",
       " \n",
       "         [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       " \n",
       " \n",
       "         [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       " \n",
       " \n",
       "         [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           ...,\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "           [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]),\n",
       " tensor([[9.5458e-01, 1.1175e-05, 3.5305e-04, 5.6990e-03, 1.4756e-04, 3.7270e-02,\n",
       "          1.2067e-03, 5.5796e-05, 6.3642e-04, 4.0918e-05],\n",
       "         [6.5945e-01, 1.0923e-03, 2.6146e-02, 8.7576e-02, 6.7032e-03, 1.0441e-01,\n",
       "          7.0507e-03, 1.8202e-02, 7.8936e-02, 1.0432e-02],\n",
       "         [9.9139e-01, 1.7938e-07, 9.5355e-05, 2.9680e-04, 2.3465e-06, 8.1085e-03,\n",
       "          3.9631e-05, 6.4136e-06, 6.2970e-05, 1.4602e-06],\n",
       "         [9.6096e-01, 6.5459e-06, 4.9963e-03, 2.7599e-03, 2.7350e-04, 1.6771e-02,\n",
       "          5.1912e-03, 1.4753e-03, 6.6036e-03, 9.6436e-04],\n",
       "         [8.8098e-01, 1.5967e-04, 3.1385e-02, 5.3548e-02, 2.4130e-04, 2.0930e-02,\n",
       "          7.6067e-04, 3.7211e-03, 7.9741e-03, 2.9538e-04],\n",
       "         [9.1703e-01, 1.7426e-05, 6.2974e-03, 7.6401e-03, 1.0310e-03, 3.8060e-02,\n",
       "          1.3019e-02, 2.7509e-03, 9.0330e-03, 5.1238e-03],\n",
       "         [6.5438e-01, 4.2774e-04, 1.3730e-02, 1.0370e-02, 6.9795e-03, 1.4660e-01,\n",
       "          1.1056e-01, 6.5577e-04, 5.4430e-02, 1.8658e-03],\n",
       "         [9.9442e-01, 2.6737e-07, 1.3506e-03, 1.7744e-04, 7.0996e-06, 3.2033e-03,\n",
       "          1.2038e-04, 7.3005e-05, 6.4239e-04, 4.6575e-06],\n",
       "         [9.9298e-01, 1.9663e-08, 3.0001e-05, 1.1278e-04, 2.8488e-07, 6.7841e-03,\n",
       "          5.3112e-06, 3.0975e-06, 8.1212e-05, 3.9032e-07],\n",
       "         [9.9446e-01, 1.3253e-07, 5.1787e-04, 1.2691e-04, 3.6535e-06, 4.4187e-03,\n",
       "          1.2020e-04, 3.9053e-06, 3.5220e-04, 8.3083e-07],\n",
       "         [9.7920e-01, 3.2438e-07, 8.9406e-05, 1.7941e-04, 8.4703e-06, 1.9513e-02,\n",
       "          7.0534e-04, 3.3649e-06, 2.9522e-04, 1.6893e-06],\n",
       "         [9.3001e-01, 5.5470e-05, 4.9951e-03, 3.9266e-04, 1.6319e-03, 8.7820e-03,\n",
       "          4.4609e-02, 6.1100e-03, 2.3399e-03, 1.0785e-03],\n",
       "         [9.8583e-01, 1.9801e-06, 1.1889e-03, 7.8980e-04, 6.7957e-05, 8.5411e-03,\n",
       "          1.4941e-03, 8.6984e-05, 1.9086e-03, 9.3882e-05],\n",
       "         [9.3222e-01, 2.0487e-04, 9.2571e-03, 2.8645e-03, 4.1949e-03, 1.5891e-02,\n",
       "          3.0136e-02, 2.4400e-03, 1.5166e-03, 1.2725e-03],\n",
       "         [4.7970e-01, 7.6277e-05, 3.4107e-03, 2.1202e-04, 1.5763e-01, 5.6237e-03,\n",
       "          3.2702e-01, 5.7354e-03, 4.1537e-03, 1.6441e-02],\n",
       "         [9.9717e-01, 2.0271e-08, 5.6934e-05, 4.3772e-05, 1.2322e-06, 2.5761e-03,\n",
       "          9.8617e-05, 6.1724e-07, 5.0227e-05, 3.9665e-07],\n",
       "         [9.9284e-01, 5.3036e-07, 1.7136e-04, 9.1372e-04, 1.1575e-05, 5.6396e-03,\n",
       "          2.4760e-04, 1.7816e-05, 1.4487e-04, 1.1723e-05],\n",
       "         [9.7982e-01, 1.4957e-06, 1.5814e-04, 5.1240e-04, 6.4595e-05, 1.6635e-02,\n",
       "          1.9086e-03, 3.0055e-05, 8.4156e-04, 2.7828e-05],\n",
       "         [9.8735e-01, 2.4707e-06, 8.2501e-04, 9.9322e-05, 8.5444e-05, 5.7128e-03,\n",
       "          5.5452e-03, 1.1339e-04, 2.1965e-04, 5.0033e-05],\n",
       "         [9.9356e-01, 3.4632e-07, 4.2591e-03, 1.7506e-04, 8.3491e-06, 1.0385e-03,\n",
       "          5.7724e-04, 1.1292e-04, 2.6398e-04, 4.7605e-06],\n",
       "         [9.2052e-01, 6.9854e-05, 4.1944e-02, 9.8668e-03, 5.7775e-05, 1.5179e-02,\n",
       "          1.2626e-03, 8.0038e-03, 2.7614e-03, 3.3855e-04],\n",
       "         [9.7498e-01, 9.7702e-07, 2.1081e-05, 6.8377e-04, 1.0890e-05, 2.4085e-02,\n",
       "          1.3128e-04, 2.2694e-05, 5.5901e-05, 5.1888e-06],\n",
       "         [9.7161e-01, 6.9781e-06, 5.0011e-03, 1.2298e-03, 4.4255e-04, 7.3306e-03,\n",
       "          1.0410e-02, 3.0919e-04, 3.3005e-03, 3.5545e-04],\n",
       "         [9.6292e-01, 1.1348e-05, 2.9474e-03, 2.4535e-04, 1.8688e-04, 2.4377e-02,\n",
       "          1.9017e-03, 9.1544e-04, 6.0806e-03, 4.1033e-04],\n",
       "         [8.2873e-01, 1.2991e-04, 7.6754e-03, 9.8648e-03, 1.4032e-03, 9.2796e-02,\n",
       "          6.3536e-03, 1.4100e-03, 4.9220e-02, 2.4141e-03],\n",
       "         [9.9342e-01, 7.0336e-07, 2.2554e-04, 4.0559e-04, 3.6544e-05, 4.4899e-03,\n",
       "          3.9930e-04, 3.8503e-05, 9.3734e-04, 4.3326e-05],\n",
       "         [6.4887e-01, 2.8992e-05, 3.2629e-02, 6.1589e-04, 1.0231e-03, 8.7626e-03,\n",
       "          3.0239e-01, 2.0788e-04, 5.4104e-03, 6.0563e-05],\n",
       "         [9.8643e-01, 1.9066e-06, 2.3996e-03, 5.0175e-03, 3.2424e-05, 4.4700e-03,\n",
       "          1.4873e-04, 2.2407e-04, 1.2338e-03, 4.0951e-05],\n",
       "         [8.4818e-01, 1.0406e-04, 6.6442e-02, 2.8654e-02, 6.8454e-05, 3.1401e-02,\n",
       "          1.3914e-04, 2.9895e-03, 2.1744e-02, 2.8195e-04],\n",
       "         [9.5612e-01, 3.4841e-05, 9.7906e-03, 1.2384e-03, 5.6886e-05, 2.6375e-02,\n",
       "          3.5989e-03, 1.1257e-03, 1.5575e-03, 1.0243e-04],\n",
       "         [8.8822e-01, 2.6495e-05, 6.8712e-03, 1.6066e-02, 9.3305e-04, 3.1502e-02,\n",
       "          8.4869e-03, 6.3779e-04, 4.5311e-02, 1.9425e-03],\n",
       "         [9.7410e-01, 3.3088e-05, 5.4788e-03, 9.5438e-04, 1.1415e-04, 8.8891e-03,\n",
       "          2.8001e-03, 4.5676e-03, 2.7895e-03, 2.7780e-04],\n",
       "         [9.8609e-01, 2.6485e-07, 1.0649e-04, 1.2681e-04, 3.2128e-06, 1.3267e-02,\n",
       "          3.0524e-04, 2.3453e-06, 9.8193e-05, 4.2157e-07],\n",
       "         [9.7769e-01, 9.3360e-06, 8.7539e-03, 1.4230e-03, 4.7173e-05, 8.2149e-03,\n",
       "          1.3109e-03, 4.3677e-04, 2.0633e-03, 5.0693e-05],\n",
       "         [7.2416e-01, 1.6058e-04, 1.1257e-03, 7.1139e-03, 2.6507e-03, 2.5619e-01,\n",
       "          5.1572e-03, 5.9248e-04, 2.3053e-03, 5.4414e-04],\n",
       "         [9.8901e-01, 7.9885e-07, 2.5852e-04, 6.8537e-04, 2.9230e-05, 7.9716e-03,\n",
       "          7.6686e-04, 6.5646e-05, 1.1534e-03, 5.4489e-05],\n",
       "         [8.5741e-01, 1.5498e-04, 9.1216e-03, 9.0898e-02, 3.0810e-04, 2.5301e-02,\n",
       "          9.7068e-04, 1.0268e-02, 4.1802e-03, 1.3854e-03],\n",
       "         [9.9210e-01, 9.6745e-07, 8.4802e-04, 2.3919e-04, 2.6819e-05, 4.3599e-03,\n",
       "          2.1406e-03, 1.5996e-05, 2.6140e-04, 6.3345e-06],\n",
       "         [8.3640e-01, 6.5263e-05, 3.9695e-02, 2.9597e-02, 3.1438e-05, 5.0446e-02,\n",
       "          1.2510e-03, 3.4842e-02, 7.3344e-03, 3.3989e-04],\n",
       "         [9.8787e-01, 5.0280e-06, 2.7372e-04, 3.4241e-04, 4.0888e-04, 7.4097e-03,\n",
       "          2.0177e-03, 1.0876e-03, 3.5352e-04, 2.3110e-04],\n",
       "         [2.2201e-04, 9.6163e-01, 1.4286e-02, 3.7044e-03, 5.2363e-04, 1.6231e-03,\n",
       "          2.3854e-03, 6.0313e-04, 1.4549e-02, 4.7044e-04],\n",
       "         [6.6741e-05, 9.7128e-01, 2.4349e-03, 6.3751e-03, 6.9481e-04, 4.7061e-03,\n",
       "          3.7219e-03, 1.1166e-03, 8.3784e-03, 1.2270e-03],\n",
       "         [2.8382e-03, 5.7356e-01, 1.0718e-01, 3.3428e-02, 2.2467e-02, 7.8018e-03,\n",
       "          7.6531e-03, 2.3307e-03, 2.3848e-01, 4.2611e-03],\n",
       "         [1.0826e-04, 9.7464e-01, 7.0166e-03, 1.9113e-03, 2.4324e-04, 1.1031e-03,\n",
       "          1.9103e-03, 2.0693e-04, 1.2598e-02, 2.6128e-04],\n",
       "         [8.0317e-04, 9.2496e-01, 1.1619e-02, 7.4468e-03, 1.6876e-03, 8.4108e-03,\n",
       "          1.0768e-02, 1.7546e-03, 2.9440e-02, 3.1127e-03],\n",
       "         [2.0348e-03, 8.1579e-01, 9.1140e-03, 1.6871e-02, 6.5544e-03, 4.7066e-02,\n",
       "          2.5089e-02, 6.0358e-03, 6.1910e-02, 9.5349e-03],\n",
       "         [4.8392e-05, 9.8330e-01, 3.6145e-03, 5.3072e-03, 4.4734e-04, 8.1216e-04,\n",
       "          1.1184e-03, 1.2728e-03, 3.3124e-03, 7.6362e-04],\n",
       "         [1.4490e-04, 9.7199e-01, 1.1262e-02, 3.1081e-03, 3.6080e-04, 1.0528e-03,\n",
       "          1.9016e-03, 4.6552e-04, 9.3233e-03, 3.9086e-04],\n",
       "         [8.2129e-04, 8.8610e-01, 1.8346e-02, 1.7047e-02, 2.9528e-03, 1.4235e-02,\n",
       "          1.7503e-02, 5.0140e-03, 3.3435e-02, 4.5470e-03],\n",
       "         [4.3382e-04, 9.2169e-01, 1.1798e-02, 2.6295e-02, 2.4290e-03, 8.9527e-03,\n",
       "          7.0386e-03, 5.9897e-03, 1.1176e-02, 4.2001e-03],\n",
       "         [3.1696e-04, 8.3048e-01, 5.9822e-02, 7.0147e-03, 1.0179e-02, 1.7527e-03,\n",
       "          4.1663e-03, 3.4085e-03, 8.0190e-02, 2.6658e-03],\n",
       "         [6.9529e-05, 9.3909e-01, 5.9936e-03, 2.7328e-02, 8.6848e-04, 6.5507e-03,\n",
       "          3.0939e-03, 2.1337e-03, 1.2792e-02, 2.0796e-03],\n",
       "         [1.0781e-03, 9.0697e-01, 3.3370e-02, 8.1837e-03, 2.3239e-03, 4.8850e-03,\n",
       "          1.2850e-02, 2.8100e-03, 2.4985e-02, 2.5490e-03],\n",
       "         [3.1924e-03, 5.2871e-02, 6.1907e-02, 1.9537e-02, 1.6889e-03, 6.4143e-02,\n",
       "          7.4780e-01, 2.9620e-03, 4.4788e-02, 1.1118e-03],\n",
       "         [1.2561e-03, 7.3336e-01, 1.4451e-02, 2.0419e-02, 7.1462e-03, 1.8823e-02,\n",
       "          4.2787e-03, 2.7363e-03, 1.9427e-01, 3.2620e-03],\n",
       "         [2.9612e-04, 9.1250e-01, 3.1321e-02, 1.4815e-02, 1.9356e-03, 1.7599e-03,\n",
       "          1.0150e-03, 3.2237e-03, 3.1718e-02, 1.4147e-03],\n",
       "         [2.0431e-04, 4.5614e-01, 1.3258e-02, 1.1208e-01, 1.2242e-02, 5.3969e-02,\n",
       "          7.2613e-03, 1.3753e-02, 2.8179e-01, 4.9302e-02],\n",
       "         [1.0741e-04, 9.3222e-01, 1.1435e-02, 7.6415e-03, 3.0188e-03, 8.8609e-04,\n",
       "          8.3598e-04, 8.6609e-03, 2.9810e-02, 5.3854e-03],\n",
       "         [9.9508e-04, 8.9595e-01, 1.5420e-02, 2.4943e-02, 5.0324e-03, 1.3746e-02,\n",
       "          1.4261e-02, 8.7407e-03, 1.3880e-02, 7.0350e-03],\n",
       "         [5.1793e-04, 5.2733e-01, 2.8246e-01, 2.4655e-02, 1.1006e-02, 1.5548e-03,\n",
       "          1.9415e-03, 9.4133e-03, 1.3767e-01, 3.4498e-03],\n",
       "         [1.6477e-03, 8.0318e-01, 5.5615e-02, 1.0586e-02, 9.7613e-04, 8.2590e-03,\n",
       "          9.5153e-03, 5.8703e-04, 1.0844e-01, 1.1884e-03],\n",
       "         [1.4965e-04, 9.6149e-01, 8.3106e-03, 8.9985e-03, 1.3421e-03, 1.6148e-03,\n",
       "          9.5232e-04, 4.7221e-03, 1.0099e-02, 2.3168e-03],\n",
       "         [1.6747e-03, 3.1754e-01, 1.4428e-01, 3.2243e-02, 3.3903e-02, 1.3356e-02,\n",
       "          3.0239e-03, 4.9042e-03, 4.4458e-01, 4.5029e-03],\n",
       "         [5.1426e-05, 9.8258e-01, 1.7823e-03, 2.2346e-03, 3.3524e-04, 1.7046e-03,\n",
       "          1.3946e-03, 6.3586e-04, 8.5580e-03, 7.1954e-04]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StudentData(Dataset):\n",
    "    def __init__(self, dataset, targets):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.targets = targets\n",
    "        self.indices = dataset.indices\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, list):\n",
    "            data, _ = self.dataset[[i for i in index]]\n",
    "            target = self.targets[[self.indices[i] for i in index]]\n",
    "        else:\n",
    "            data, _ = self.dataset[index]\n",
    "            target = self.targets[self.indices[index]]\n",
    "            \n",
    "        return data, target\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Total number of samples\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "student_data = StudentData(public_data, student_targets)\n",
    "student_data_loader = DataLoader(student_data, public_batch_size)\n",
    "data_iter = iter(student_data_loader)\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329b1791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: Train loss: 0.018\r"
     ]
    }
   ],
   "source": [
    "model = Mnist_Student()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "n_epochs_student = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs_student):\n",
    "    train_loss = []\n",
    "    for x, y in student_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs_student}: \"\n",
    "          f\"Train loss: {sum(train_loss)/len(train_loss):.3f}\", end=\"\\r\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32c5544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.762\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "     for x, y in test_data_loader:\n",
    "            output = model(x)\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                total +=1\n",
    "                \n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f616dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = \"fedavg+feded\"\n",
    "algs = alg.split(\"+\")\n",
    "ens = [\"fede\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6977e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "any(alg in algs for alg in ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d1d9c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983b2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
